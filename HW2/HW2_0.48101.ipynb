{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import *\n",
    "from keras.layers import Embedding,Bidirectional\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten,GlobalAveragePooling1D\n",
    "from keras.layers import SpatialDropout1D,LSTM,BatchNormalization,Dropout\n",
    "from keras.layers import merge,concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corrine1106/.conda/envs/py3.7/lib/python3.7/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for plural nuons\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-86979574ef63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py3.7/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop = stop-set(['what',\n",
    " 'when',\n",
    " 'where',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whom',\n",
    " 'over'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train,test])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Golden, gaudy and glorious: Dubai has the world's tallest building and biggest airport... is it about to overtake London as the most visited city?\""
      ]
     },
     "execution_count": 1118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Headline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport = ['sport', 'football',  'golf', 'rugbyunion', 'boxing', 'tennis','othersports']\n",
    "non_sport = ['travel', 'health', 'femail', 'gardening', 'sciencetech',\n",
    "       'news', 'food',  'travelnews', 'cricket', \n",
    "       'books', 'home', 'concussion','beauty', 'formulaone', 'racing', 'living',\n",
    "       'middleeast', 'us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport = pd.DataFrame([])\n",
    "for i in sport:\n",
    "    df_sport = df_sport.append(df[df['Category'] == i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_sport = pd.DataFrame([])\n",
    "for i in non_sport:\n",
    "    df_non_sport = df_non_sport.append(df[df['Category'] == i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean puctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_sport['Headline']=df_non_sport['Headline'].apply(lambda x : x.replace(\"'s\",' '))\n",
    "df_non_sport['Headline']=df_non_sport['Headline'].apply(lambda x : x.replace(\"s'\",' '))\n",
    "df_sport['Headline']=df_sport['Headline'].apply(lambda x : x.replace(\"'s\",' '))\n",
    "df_sport['Headline']=df_sport['Headline'].apply(lambda x : x.replace(\"s'\",' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Luis Suarez  bite has ensured his legacy as a ...</td>\n",
       "      <td>sport</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>Premier League top 10 debuts: Diego Costa, Ces...</td>\n",
       "      <td>sport</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>220</td>\n",
       "      <td>Yoann Huget warned by The European Professiona...</td>\n",
       "      <td>sport</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>315</td>\n",
       "      <td>Dani Alves reaches 300 appearances for Barcelo...</td>\n",
       "      <td>sport</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>339</td>\n",
       "      <td>Andre-Pierre Gignac  transfer from Toulouse to...</td>\n",
       "      <td>sport</td>\n",
       "      <td>2.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                           Headline Category     Label\n",
       "4      5  Luis Suarez  bite has ensured his legacy as a ...    sport  4.000000\n",
       "59    60  Premier League top 10 debuts: Diego Costa, Ces...    sport  3.000000\n",
       "219  220  Yoann Huget warned by The European Professiona...    sport  2.000000\n",
       "314  315  Dani Alves reaches 300 appearances for Barcelo...    sport  2.000000\n",
       "338  339  Andre-Pierre Gignac  transfer from Toulouse to...    sport  2.333333"
      ]
     },
     "execution_count": 1123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1124,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '\"#$%&\\'()*+,./:;<=>@[\\\\]^_`{|}~!?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry i spent it on myself Harvey Nichols hilarious Christmas advert sees people treating themselves instead of others\n"
     ]
    }
   ],
   "source": [
    "example=\"Sorry, i spent it on myself! Harvey Nichol's hilarious Christmas advert sees people treating themselves instead of others\"\n",
    "print(remove_punct(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### football club"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "metadata": {},
   "outputs": [],
   "source": [
    "football_club_list = ['Alavés','Athletic Bilbao','Atlético Madrid','Barcelona'\n",
    "                      ,'Cádiz','Celta Vigo','Eibar','Elche','Getafe','Granada'\n",
    "                     ,'Huesca','Levante','Osasuna','Real Betis','Real Madrid','Real Sociedad','Sevilla'\n",
    "                     ,'Valencia','Valladolid','Villarreal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def football_club(title):\n",
    "    word_list = title.split(' ')\n",
    "    for word in word_list:\n",
    "        if word in football_club_list:\n",
    "            title = title.replace(word,'football club')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport['Headline']=df_sport['Headline'].apply(lambda x : football_club(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "metadata": {},
   "outputs": [],
   "source": [
    "footballer_list = ['Paul Pogba','Dejan Lovren']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def footballer(title):\n",
    "    word_list = title.split(' ')\n",
    "    for word in word_list:\n",
    "        if word in footballer_list:\n",
    "            title = title.replace(word,'footballer')\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport['Headline']=df_sport['Headline'].apply(lambda x : footballer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['£ 180', '£ 5']"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"£ \\d*\\,?\\d+\\.?\\d*\",df.Headline[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_money(title):\n",
    "    money_list = re.findall(r\"£ \\d*\\,?\\d+\\.?\\d*\",title)\n",
    "    if not money_list:\n",
    "        return title\n",
    "    else:\n",
    "        for money in money_list:\n",
    "            title = title.replace(money, 'money')\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_name(title):\n",
    "    doc = nlp(title)\n",
    "    if not doc.ents :\n",
    "        return title\n",
    "    else:\n",
    "        for X in doc.ents:\n",
    "            if X.label_=='PERSON':\n",
    "                title = title.replace(X.text, ' ')\n",
    "                \n",
    "    return title\n",
    "            \n",
    "def remove_date(title):\n",
    "    doc = nlp(title)\n",
    "    if not doc.ents :\n",
    "        return title\n",
    "    else:\n",
    "        for X in doc.ents:\n",
    "            if X.label_=='DATE':\n",
    "                title = title.replace(X.text, ' ')\n",
    "                \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_name(title):\n",
    "    doc = nlp(title)\n",
    "    if not doc.ents :\n",
    "        return title\n",
    "    else:\n",
    "        for X in doc.ents:\n",
    "            if X.label_=='PERSON':\n",
    "                print(title)\n",
    "                title = title.replace(X.text, 'athlete')\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sport['Headline']=df_sport['Headline'].apply(lambda x : remove_name(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_sport.append(df_non_sport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headline']=df['Headline'].apply(lambda x : remove_date(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headline']=df['Headline'].apply(lambda x : replace_money(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Headline']=df['Headline'].apply(lambda x : remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Golden gaudy and glorious Dubai has the world  tallest building and biggest airport is it about to overtake London as the most visited city'"
      ]
     },
     "execution_count": 1143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Headline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stop = set([\"would\",'could','new','hi','say','world','reveal','money','first','one','reveals','says','over'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['Headline'].apply(lambda x : remove_punct(x))\n",
    "df['tokenized'] = df['Headline'].apply(word_tokenize)\n",
    "\n",
    "\n",
    "\n",
    "df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "df['no_stopwords'] = df['lower'].apply(lambda x: [word for word in x if word not in new_stop]) #if word not in new_stop\n",
    "df['no_stopwords'] = df['no_stopwords'].apply(lambda x: [word for word in x if word not in other_stop])\n",
    "df['no_stopwords'] = [' '.join(map(str, l)) for l in df['no_stopwords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_stopwords'] = df['no_stopwords'].apply(lambda x: x.replace(\"polouse\", 'scenery'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_list = []\n",
    "for label in df.Label.unique():\n",
    "    if np.isnan(label) == False:\n",
    "        df_label_list.append(df[df['Label'] == label])\n",
    "label_top = defaultdict(list)\n",
    "for df_label in df_label_list:\n",
    "    corpus = create_corpus(df_label,'no_stopwords')\n",
    "    dic=defaultdict(int)\n",
    "    for word in corpus:\n",
    "        dic[word]+=1\n",
    "        \n",
    "    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n",
    "    label_top[df_label.Label.unique()[0]] = top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus(df,'Headline')\n",
    "dic=defaultdict(int)\n",
    "for word in corpus:\n",
    "    dic[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter=Counter(corpus)\n",
    "most=counter.most_common()\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in most[:30]:\n",
    "    x.append(word)\n",
    "    y.append(count)\n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 1304),\n",
       " ('to', 1179),\n",
       " ('of', 897),\n",
       " ('and', 885),\n",
       " ('a', 847),\n",
       " ('in', 846),\n",
       " ('for', 517),\n",
       " ('with', 384),\n",
       " ('on', 382),\n",
       " ('as', 331),\n",
       " ('is', 277),\n",
       " ('after', 263),\n",
       " ('at', 246),\n",
       " ('by', 219),\n",
       " ('from', 218),\n",
       " ('that', 200),\n",
       " ('it', 195),\n",
       " ('The', 192),\n",
       " ('you', 185),\n",
       " ('her', 182),\n",
       " ('are', 169),\n",
       " ('who', 154),\n",
       " ('be', 152),\n",
       " ('his', 149),\n",
       " ('but', 134),\n",
       " ('your', 133),\n",
       " ('new', 123),\n",
       " ('have', 122),\n",
       " ('up', 118),\n",
       " ('out', 117),\n",
       " ('could', 104),\n",
       " ('money', 103),\n",
       " ('he', 101),\n",
       " ('can', 100),\n",
       " ('she', 100),\n",
       " ('world', 96),\n",
       " ('was', 95),\n",
       " ('first', 91),\n",
       " ('will', 88),\n",
       " ('their', 88),\n",
       " ('has', 87),\n",
       " ('they', 85),\n",
       " ('an', 84),\n",
       " ('reveals', 80),\n",
       " ('one', 80),\n",
       " ('more', 79),\n",
       " ('how', 79),\n",
       " ('just', 74),\n",
       " ('says', 73),\n",
       " ('most', 69),\n",
       " ('over', 69),\n",
       " ('its', 69),\n",
       " ('not', 68),\n",
       " ('A', 68),\n",
       " ('than', 67),\n",
       " ('into', 64),\n",
       " ('or', 63),\n",
       " ('all', 62),\n",
       " ('get', 56),\n",
       " ('off', 56),\n",
       " ('Is', 55),\n",
       " ('being', 55),\n",
       " ('people', 54),\n",
       " ('best', 54),\n",
       " ('i', 53),\n",
       " ('life', 53),\n",
       " ('time', 53),\n",
       " ('back', 51),\n",
       " ('London', 50),\n",
       " ('cancer', 50),\n",
       " ('so', 49),\n",
       " ('this', 48),\n",
       " ('like', 47),\n",
       " ('make', 46),\n",
       " ('women', 46),\n",
       " ('claims', 46),\n",
       " ('woman', 46),\n",
       " ('man', 46),\n",
       " ('United', 45),\n",
       " ('no', 45),\n",
       " ('How', 45),\n",
       " ('made', 44),\n",
       " ('my', 43),\n",
       " ('New', 43),\n",
       " ('against', 43),\n",
       " ('say', 43),\n",
       " ('Manchester', 43),\n",
       " ('even', 42),\n",
       " ('love', 42),\n",
       " ('when', 42),\n",
       " ('food', 41),\n",
       " ('about', 39),\n",
       " ('what', 39),\n",
       " ('I', 39),\n",
       " ('before', 39),\n",
       " ('our', 38),\n",
       " ('top', 38),\n",
       " ('home', 38),\n",
       " ('reveal', 38),\n",
       " ('Uk', 37),\n",
       " ('during', 37),\n",
       " ('day', 37),\n",
       " ('three', 37),\n",
       " ('family', 37),\n",
       " ('Star', 37),\n",
       " ('British', 36),\n",
       " ('now', 36),\n",
       " ('help', 36),\n",
       " ('two', 36),\n",
       " ('them', 36),\n",
       " ('eat', 35),\n",
       " ('mother', 35),\n",
       " ('go', 35),\n",
       " ('England', 35),\n",
       " ('dinner', 35),\n",
       " ('revealed', 35),\n",
       " ('set', 35),\n",
       " ('why', 35),\n",
       " ('would', 34),\n",
       " ('dont', 34),\n",
       " ('left', 33),\n",
       " ('found', 33),\n",
       " ('may', 33),\n",
       " ('we', 33),\n",
       " ('had', 33),\n",
       " ('holiday', 33),\n",
       " ('children', 33),\n",
       " ('restaurant', 33),\n",
       " ('Britain', 33),\n",
       " ('him', 32),\n",
       " ('if', 32),\n",
       " ('down', 32),\n",
       " ('where', 32),\n",
       " ('because', 32),\n",
       " ('men', 31),\n",
       " ('were', 31),\n",
       " ('take', 31),\n",
       " ('scientists', 31),\n",
       " ('way', 31),\n",
       " ('hotel', 31),\n",
       " ('face', 30),\n",
       " ('too', 30),\n",
       " ('still', 30),\n",
       " ('been', 30),\n",
       " ('while', 30),\n",
       " ('video', 29),\n",
       " ('study', 29),\n",
       " ('goes', 29),\n",
       " ('inside', 29),\n",
       " ('World', 29),\n",
       " ('risk', 29),\n",
       " ('football', 29),\n",
       " ('there', 29),\n",
       " ('ever', 28),\n",
       " ('me', 28),\n",
       " ('do', 28),\n",
       " ('only', 28),\n",
       " ('baby', 28),\n",
       " ('own', 28),\n",
       " ('death', 28),\n",
       " ('creates', 27),\n",
       " ('League', 27),\n",
       " ('pay', 27),\n",
       " ('really', 27),\n",
       " ('sex', 27),\n",
       " ('10', 26),\n",
       " ('girl', 26),\n",
       " ('cant', 26),\n",
       " ('police', 26),\n",
       " ('look', 26),\n",
       " ('good', 26),\n",
       " ('doctors', 26),\n",
       " ('city', 25),\n",
       " ('other', 25),\n",
       " ('It', 25),\n",
       " ('car', 25),\n",
       " ('m', 25),\n",
       " ('stars', 25),\n",
       " ('Why', 25),\n",
       " ('using', 24),\n",
       " ('away', 24),\n",
       " ('play', 24),\n",
       " ('disease', 24),\n",
       " ('red', 24),\n",
       " ('City', 24),\n",
       " ('live', 24),\n",
       " ('heart', 24),\n",
       " ('save', 24),\n",
       " ('Chelsea', 23),\n",
       " ('fans', 23),\n",
       " ('wine', 23),\n",
       " ('brain', 23),\n",
       " ('water', 23),\n",
       " ('give', 22),\n",
       " ('never', 22),\n",
       " ('luxury', 22),\n",
       " ('--', 22),\n",
       " ('tv', 22),\n",
       " ('shows', 22),\n",
       " ('surgery', 22),\n",
       " ('club', 22),\n",
       " ('ice', 22),\n",
       " ('health', 22),\n",
       " ('earth', 22),\n",
       " ('half', 21),\n",
       " ('want', 21),\n",
       " ('Could', 21),\n",
       " ('stop', 21),\n",
       " ('every', 21),\n",
       " ('under', 21),\n",
       " ('gives', 21),\n",
       " ('takes', 21),\n",
       " ('end', 21),\n",
       " ('times', 21),\n",
       " ('York', 21),\n",
       " ('four', 21),\n",
       " ('which', 20),\n",
       " ('very', 20),\n",
       " ('Madrid', 20),\n",
       " ('stunning', 20),\n",
       " ('son', 20),\n",
       " ('Cup', 20),\n",
       " ('charity', 20),\n",
       " ('favourite', 20),\n",
       " ('secret', 20),\n",
       " ('hit', 20),\n",
       " ('show', 20),\n",
       " ('told', 20),\n",
       " ('does', 20),\n",
       " ('run', 20),\n",
       " ('sale', 20),\n",
       " ('us', 19),\n",
       " ('put', 19),\n",
       " ('condition', 19),\n",
       " ('Forget', 19),\n",
       " ('fashion', 19),\n",
       " ('real', 19),\n",
       " ('ahead', 19),\n",
       " ('find', 19),\n",
       " ('sign', 19),\n",
       " ('husband', 19),\n",
       " ('dead', 19),\n",
       " ('including', 19),\n",
       " ('expensive', 19),\n",
       " ('through', 18),\n",
       " ('America', 18),\n",
       " ('Premier', 18),\n",
       " ('wife', 18),\n",
       " ('boss', 18),\n",
       " ('tea', 18),\n",
       " ('skin', 18),\n",
       " ('Arsenal', 18),\n",
       " ('attack', 18),\n",
       " ('future', 18),\n",
       " ('win', 18),\n",
       " ('year', 18),\n",
       " ('celebrity', 18),\n",
       " ('daughter', 18),\n",
       " ('makes', 18),\n",
       " ('likely', 18),\n",
       " ('use', 18),\n",
       " ('boy', 18),\n",
       " ('little', 18),\n",
       " ('chance', 18),\n",
       " ('big', 18),\n",
       " ('body', 18),\n",
       " ('million', 17),\n",
       " ('star', 17),\n",
       " ('gets', 17),\n",
       " ('drug', 17),\n",
       " ('launches', 17),\n",
       " ('Real', 17),\n",
       " ('looks', 17),\n",
       " ('online', 17),\n",
       " ('Prince', 17),\n",
       " ('Apple', 17),\n",
       " ('images', 17),\n",
       " ('Google', 17),\n",
       " ('between', 17),\n",
       " ('gold', 17),\n",
       " ('hospital', 17),\n",
       " ('Kate', 17),\n",
       " ('killed', 17),\n",
       " ('return', 17),\n",
       " ('An', 17),\n",
       " ('Us', 17),\n",
       " ('great', 17),\n",
       " ('internet', 16),\n",
       " ('Liverpool', 16),\n",
       " ('meet', 16),\n",
       " ('moment', 16),\n",
       " ('should', 16),\n",
       " ('West', 16),\n",
       " ('better', 16),\n",
       " ('Now', 16),\n",
       " ('hot', 16),\n",
       " ('planet', 16),\n",
       " ('patients', 16),\n",
       " ('air', 16),\n",
       " ('head', 16),\n",
       " ('again', 16),\n",
       " ('parents', 16),\n",
       " ('calls', 16),\n",
       " ('without', 16),\n",
       " ('lost', 16),\n",
       " ('around', 16),\n",
       " ('Facebook', 16),\n",
       " ('phone', 16),\n",
       " ('despite', 16),\n",
       " ('birth', 16),\n",
       " ('style', 16),\n",
       " ('become', 15),\n",
       " ('experts', 15),\n",
       " ('due', 15),\n",
       " ('old', 15),\n",
       " ('blood', 15),\n",
       " ('From', 15),\n",
       " ('sea', 15),\n",
       " ('eye', 15),\n",
       " ('side', 15),\n",
       " ('book', 15),\n",
       " ('island', 15),\n",
       " ('much', 15),\n",
       " ('need', 15),\n",
       " ('hair', 15),\n",
       " ('child', 15),\n",
       " ('power', 15),\n",
       " ('beauty', 15),\n",
       " ('see', 15),\n",
       " ('service', 15),\n",
       " ('pregnant', 15),\n",
       " ('chocolate', 15),\n",
       " ('used', 15),\n",
       " ('French', 15),\n",
       " ('arsenal', 15),\n",
       " ('finally', 15),\n",
       " ('30', 15),\n",
       " ('night', 15),\n",
       " ('worst', 14),\n",
       " ('almost', 14),\n",
       " ('Would', 14),\n",
       " ('light', 14),\n",
       " ('20', 14),\n",
       " ('virus', 14),\n",
       " ('giant', 14),\n",
       " ('perfect', 14),\n",
       " ('team', 14),\n",
       " ('forced', 14),\n",
       " ('human', 14),\n",
       " ('eating', 14),\n",
       " ('doesnt', 14),\n",
       " ('meals', 14),\n",
       " ('claim', 14),\n",
       " ('cake', 14),\n",
       " ('Turkey', 14),\n",
       " ('wedding', 14),\n",
       " ('create', 14),\n",
       " ('right', 14),\n",
       " ('come', 14),\n",
       " ('opens', 14),\n",
       " ('model', 14),\n",
       " ('Nhs', 14),\n",
       " ('Kim', 14),\n",
       " ('walk', 14),\n",
       " ('burger', 14),\n",
       " ('weight', 14),\n",
       " ('size', 14),\n",
       " ('six', 14),\n",
       " ('know', 14),\n",
       " ('many', 14),\n",
       " ('wont', 14),\n",
       " ('change', 14),\n",
       " ('drink', 13),\n",
       " ('full', 13),\n",
       " ('match', 13),\n",
       " ('record', 13),\n",
       " ('cup', 13),\n",
       " ('rare', 13),\n",
       " ('twice', 13),\n",
       " ('taking', 13),\n",
       " ('history', 13),\n",
       " ('since', 13),\n",
       " ('Europe', 13),\n",
       " ('treatment', 13),\n",
       " ('space', 13),\n",
       " ('striker', 13),\n",
       " ('bid', 13),\n",
       " ('fish', 13),\n",
       " ('50', 13),\n",
       " ('helps', 13),\n",
       " ('girls', 13),\n",
       " ('cut', 13),\n",
       " ('high', 13),\n",
       " ('technology', 13),\n",
       " ('front', 13),\n",
       " ('making', 13),\n",
       " ('once', 13),\n",
       " ('visit', 13),\n",
       " ('school', 13),\n",
       " ('comes', 13),\n",
       " ('nothing', 13),\n",
       " ('game', 13),\n",
       " ('behind', 13),\n",
       " ('five', 13),\n",
       " ('finds', 13),\n",
       " ('3d', 12),\n",
       " ('Britons', 12),\n",
       " ('league', 12),\n",
       " ('view', 12),\n",
       " ('hours', 12),\n",
       " ('having', 12),\n",
       " ('complete', 12),\n",
       " ('calories', 12),\n",
       " ('call', 12),\n",
       " ('American', 12),\n",
       " ('Iphone', 12),\n",
       " ('battle', 12),\n",
       " ('long', 12),\n",
       " ('views', 12),\n",
       " ('build', 12),\n",
       " ('fat', 12),\n",
       " ('What', 12),\n",
       " ('plans', 12),\n",
       " ('admits', 12),\n",
       " ('campaign', 12),\n",
       " ('ve', 12),\n",
       " ('former', 12),\n",
       " ('Spain', 12),\n",
       " ('inspector', 12),\n",
       " ('open', 12),\n",
       " ('getting', 12),\n",
       " ('young', 12),\n",
       " ('Kardashian', 12),\n",
       " ('injury', 12),\n",
       " ('deal', 12),\n",
       " ('work', 12),\n",
       " ('lose', 12),\n",
       " ('Newcastle', 12),\n",
       " ('flying', 12),\n",
       " ('going', 12),\n",
       " ('bizarre', 12),\n",
       " ('photos', 12),\n",
       " ('crowned', 12),\n",
       " ('keep', 12),\n",
       " ('clash', 12),\n",
       " ('enough', 12),\n",
       " ('beer', 12),\n",
       " ('did', 12),\n",
       " ('got', 12),\n",
       " ('suffer', 12),\n",
       " ('Nasa', 12),\n",
       " ('biggest', 11),\n",
       " ('think', 11),\n",
       " ('Will', 11),\n",
       " ('female', 11),\n",
       " ('break', 11),\n",
       " ('far', 11),\n",
       " ('fine', 11),\n",
       " ('thought', 11),\n",
       " ('firm', 11),\n",
       " ('became', 11),\n",
       " ('among', 11),\n",
       " ('stone', 11),\n",
       " ('6', 11),\n",
       " ('travel', 11),\n",
       " ('amazing', 11),\n",
       " ('same', 11),\n",
       " ('guide', 11),\n",
       " ('hits', 11),\n",
       " ('latest', 11),\n",
       " ('app', 11),\n",
       " ('cream', 11),\n",
       " ('system', 11),\n",
       " ('officer', 11),\n",
       " ('Ebola', 11),\n",
       " ('100', 11),\n",
       " ('mystery', 11),\n",
       " ('across', 11),\n",
       " ('breakfast', 11),\n",
       " ('fast', 11),\n",
       " ('Indian', 11),\n",
       " ('incredible', 11),\n",
       " ('warns', 11),\n",
       " ('France', 11),\n",
       " ('wearing', 11),\n",
       " ('milk', 11),\n",
       " ('Mother', 11),\n",
       " ('raise', 11),\n",
       " ('ancient', 11),\n",
       " ('hand', 11),\n",
       " ('price', 11),\n",
       " ('shot', 11),\n",
       " ('festive', 11),\n",
       " ('then', 11),\n",
       " ('Chinese', 11),\n",
       " ('part', 11),\n",
       " ('Woman', 11),\n",
       " ('Dont', 10),\n",
       " ('leave', 10),\n",
       " ('lives', 10),\n",
       " ('move', 10),\n",
       " ('happy', 10),\n",
       " ('Red', 10),\n",
       " ('Tottenham', 10),\n",
       " ('here', 10),\n",
       " ('sun', 10),\n",
       " ('greatest', 10),\n",
       " ('Mars', 10),\n",
       " ('Harry', 10),\n",
       " ('buy', 10),\n",
       " ('murder', 10),\n",
       " ('German', 10),\n",
       " ('fruit', 10),\n",
       " ('healthy', 10),\n",
       " ('place', 10),\n",
       " ('discover', 10),\n",
       " ('round', 10),\n",
       " ('South', 10),\n",
       " ('close', 10),\n",
       " ('moving', 10),\n",
       " ('boost', 10),\n",
       " ('some', 10),\n",
       " ('friends', 10),\n",
       " ('range', 10),\n",
       " ('student', 10),\n",
       " ('chips', 10),\n",
       " ('let', 10),\n",
       " ('diet', 10),\n",
       " ('rules', 10),\n",
       " ('Brazil', 10),\n",
       " ('No', 10),\n",
       " ('George', 10),\n",
       " ('meal', 10),\n",
       " ('unveils', 10),\n",
       " ('plan', 10),\n",
       " ('double', 10),\n",
       " ('fight', 10),\n",
       " ('Australian', 10),\n",
       " ('following', 10),\n",
       " ('offers', 10),\n",
       " ('inspired', 10),\n",
       " ('House', 10),\n",
       " ('couple', 10),\n",
       " ('There', 10),\n",
       " ('William', 10),\n",
       " ('robot', 10),\n",
       " ('these', 10),\n",
       " ('eggs', 10),\n",
       " ('lets', 10),\n",
       " ('obese', 10),\n",
       " ('turned', 10),\n",
       " ('any', 10),\n",
       " ('seen', 10),\n",
       " ('birthday', 10),\n",
       " ('menu', 10),\n",
       " ('beach', 10),\n",
       " ('David', 10),\n",
       " ('diagnosed', 10),\n",
       " ('beautiful', 10),\n",
       " ('test', 10),\n",
       " ('babies', 10),\n",
       " ('becomes', 10),\n",
       " ('insists', 10),\n",
       " ('tour', 10),\n",
       " ('black', 10),\n",
       " ('minutes', 10),\n",
       " ('glorious', 9),\n",
       " ('tourists', 9),\n",
       " ('prevent', 9),\n",
       " ('victory', 9),\n",
       " ('die', 9),\n",
       " ('final', 9),\n",
       " ('exercise', 9),\n",
       " ('storm', 9),\n",
       " ('leaves', 9),\n",
       " ('wants', 9),\n",
       " ('bad', 9),\n",
       " ('climate', 9),\n",
       " ('Champions', 9),\n",
       " ('First', 9),\n",
       " ('apple', 9),\n",
       " ('royal', 9),\n",
       " ('giving', 9),\n",
       " ('cells', 9),\n",
       " ('war', 9),\n",
       " ('state', 9),\n",
       " ('list', 9),\n",
       " ('cook', 9),\n",
       " ('Stuart', 9),\n",
       " ('glamour', 9),\n",
       " ('watch', 9),\n",
       " ('Scotland', 9),\n",
       " ('cure', 9),\n",
       " ('cost', 9),\n",
       " ('number', 9),\n",
       " ('Caribbean', 9),\n",
       " ('wins', 9),\n",
       " ('diners', 9),\n",
       " ('lead', 9),\n",
       " ('News', 9),\n",
       " ('dog', 9),\n",
       " ('frozen', 9),\n",
       " ('suffering', 9),\n",
       " ('bill', 9),\n",
       " ('well', 9),\n",
       " ('wrong', 9),\n",
       " ('photo', 9),\n",
       " ('thanks', 9),\n",
       " ('Africa', 9),\n",
       " ('both', 9),\n",
       " ('Duchess', 9),\n",
       " ('We', 9),\n",
       " ('Man', 9),\n",
       " ('treat', 9),\n",
       " ('free', 9),\n",
       " ('journey', 9),\n",
       " ('forward', 9),\n",
       " ('thousands', 9),\n",
       " ('users', 9),\n",
       " ('families', 9),\n",
       " ('few', 9),\n",
       " ('M', 9),\n",
       " ('looking', 9),\n",
       " ('Paris', 9),\n",
       " ('loses', 9),\n",
       " ('week', 9),\n",
       " ('steps', 9),\n",
       " ('born', 9),\n",
       " ('Sir', 9),\n",
       " ('film', 9),\n",
       " ('actually', 9),\n",
       " ('feel', 9),\n",
       " ('stay', 9),\n",
       " ('passengers', 9),\n",
       " ('turns', 9),\n",
       " ('warning', 9),\n",
       " ('those', 9),\n",
       " ('arrested', 9),\n",
       " ('Queen', 9),\n",
       " ('ill', 9),\n",
       " ('discovers', 9),\n",
       " ('lunch', 9),\n",
       " ('killer', 9),\n",
       " ('given', 9),\n",
       " ('spot', 9),\n",
       " ('dream', 9),\n",
       " ('art', 9),\n",
       " ('partner', 8),\n",
       " ('Can', 8),\n",
       " ('artist', 8),\n",
       " ('expert', 8),\n",
       " ('marriage', 8),\n",
       " ('warn', 8),\n",
       " ('ban', 8),\n",
       " ('age', 8),\n",
       " ('finish', 8),\n",
       " ('developing', 8),\n",
       " ('nearly', 8),\n",
       " ('didnt', 8),\n",
       " ('search', 8),\n",
       " ('Scottish', 8),\n",
       " ('serves', 8),\n",
       " ('per', 8),\n",
       " ('tourist', 8),\n",
       " ('early', 8),\n",
       " ('chef', 8),\n",
       " ('shock', 8),\n",
       " ('winner', 8),\n",
       " ('Valentine', 8),\n",
       " ('Obama', 8),\n",
       " ('weekend', 8),\n",
       " ('race', 8),\n",
       " ('24', 8),\n",
       " ('third', 8),\n",
       " ('electric', 8),\n",
       " ('mothers', 8),\n",
       " ('Brits', 8),\n",
       " ('Brussels', 8),\n",
       " ('Father', 8),\n",
       " ('taken', 8),\n",
       " ('report', 8),\n",
       " ('saved', 8),\n",
       " ('Ham', 8),\n",
       " ('players', 8),\n",
       " ('smart', 8),\n",
       " ('lingerie', 8),\n",
       " ('must', 8),\n",
       " ('aged', 8),\n",
       " ('He', 8),\n",
       " ('gave', 8),\n",
       " ('guests', 8),\n",
       " ('Exclusive', 8),\n",
       " ('owner', 8),\n",
       " ('catwalk', 8),\n",
       " ('tests', 8),\n",
       " ('Police', 8),\n",
       " ('blame', 8),\n",
       " ('That', 8),\n",
       " ('energy', 8),\n",
       " ('camera', 8),\n",
       " ('plus', 8),\n",
       " ('Baby', 8),\n",
       " ('Tv', 8),\n",
       " ('met', 8),\n",
       " ('Mr', 8),\n",
       " ('brand', 8),\n",
       " ('crisis', 8),\n",
       " ('music', 8),\n",
       " ('dangerous', 8),\n",
       " ('tax', 8),\n",
       " ('sleep', 8),\n",
       " ('second', 8),\n",
       " ('feast', 8),\n",
       " ('doing', 8),\n",
       " ('Palace', 8),\n",
       " ('needs', 8),\n",
       " ('fire', 8),\n",
       " ('queen', 8),\n",
       " ('models', 8),\n",
       " ('James', 8),\n",
       " ('Alzheimer', 8),\n",
       " ('glass', 8),\n",
       " ('hour', 8),\n",
       " ('Victoria', 8),\n",
       " ('tell', 8),\n",
       " ('vs', 8),\n",
       " ('uses', 8),\n",
       " ('featuring', 8),\n",
       " ('lived', 8),\n",
       " ('tips', 8),\n",
       " ('East', 8),\n",
       " ('femail', 8),\n",
       " ('waste', 8),\n",
       " ('fit', 8),\n",
       " ('house', 8),\n",
       " ('animal', 8),\n",
       " ('Twitter', 8),\n",
       " ('send', 8),\n",
       " ('goal', 8),\n",
       " ('beaches', 8),\n",
       " ('train', 8),\n",
       " ('dress', 8),\n",
       " ('trial', 8),\n",
       " ('office', 8),\n",
       " ('start', 8),\n",
       " ('crash', 8),\n",
       " ('cold', 8),\n",
       " ('designer', 8),\n",
       " ('promises', 8),\n",
       " ('government', 8),\n",
       " ('Leicester', 8),\n",
       " ('playing', 8),\n",
       " ('bar', 8),\n",
       " ('kids', 8),\n",
       " ('airport', 7),\n",
       " ('spent', 7),\n",
       " ('sees', 7),\n",
       " ('mobile', 7),\n",
       " ('tells', 7),\n",
       " ('benefits', 7),\n",
       " ('secrets', 7),\n",
       " ('romantic', 7),\n",
       " ('name', 7),\n",
       " ('breast', 7),\n",
       " ('challenge', 7),\n",
       " ('strong', 7),\n",
       " ('wanted', 7),\n",
       " ('role', 7),\n",
       " ('apart', 7),\n",
       " ('bacteria', 7),\n",
       " ('anyone', 7),\n",
       " ('newest', 7),\n",
       " ('Hotel', 7),\n",
       " ('pictures', 7),\n",
       " ('wear', 7),\n",
       " ('court', 7),\n",
       " ('morning', 7),\n",
       " ('working', 7),\n",
       " ('Isis', 7),\n",
       " ('believe', 7),\n",
       " ('12', 7),\n",
       " ('captured', 7),\n",
       " ('Sydney', 7),\n",
       " ('entirely', 7),\n",
       " ('scenery', 7),\n",
       " ('travellers', 7),\n",
       " ('living', 7),\n",
       " ('past', 7),\n",
       " ('surface', 7),\n",
       " ('trying', 7),\n",
       " ('drinks', 7),\n",
       " ('North', 7),\n",
       " ('Michael', 7),\n",
       " ('Want', 7),\n",
       " ('try', 7),\n",
       " ('next', 7),\n",
       " ('rival', 7),\n",
       " ('coming', 7),\n",
       " ('moon', 7),\n",
       " ('drive', 7),\n",
       " ('until', 7),\n",
       " ('last', 7),\n",
       " ('bird', 7),\n",
       " ('capital', 7),\n",
       " ('juice', 7),\n",
       " ('sprouts', 7),\n",
       " ('middle', 7),\n",
       " ('also', 7),\n",
       " ('streets', 7),\n",
       " ('transfer', 7),\n",
       " ('class', 7),\n",
       " ('ring', 7),\n",
       " ('someone', 7),\n",
       " ('road', 7),\n",
       " ('yet', 7),\n",
       " ('straight', 7),\n",
       " ('order', 7),\n",
       " ('Australia', 7),\n",
       " ('boys', 7),\n",
       " ('obesity', 7),\n",
       " ('doctor', 7),\n",
       " ('costs', 7),\n",
       " ('plastic', 7),\n",
       " ('Syria', 7),\n",
       " ('speed', 7),\n",
       " ('title', 7),\n",
       " ('ten', 7),\n",
       " ('suffers', 7),\n",
       " ('public', 7),\n",
       " ('driving', 7),\n",
       " ('drinking', 7),\n",
       " ('When', 7),\n",
       " ('problem', 7),\n",
       " ('street', 7),\n",
       " ('went', 7),\n",
       " ('captures', 7),\n",
       " ('tried', 7),\n",
       " ('luxurious', 7),\n",
       " ('celebrates', 7),\n",
       " ('offer', 7),\n",
       " ('tumour', 7),\n",
       " ('simple', 7),\n",
       " ('western', 7),\n",
       " ('2', 7),\n",
       " ('roast', 7),\n",
       " ('St', 7),\n",
       " ('legend', 7),\n",
       " ('bring', 7),\n",
       " ('company', 7),\n",
       " ('country', 7),\n",
       " ('beats', 7),\n",
       " ('faces', 7),\n",
       " ('Just', 7),\n",
       " ('Mcdonald', 7),\n",
       " ('kitchen', 7),\n",
       " ('My', 7),\n",
       " ('champagne', 7),\n",
       " ('story', 7),\n",
       " ('quirky', 7),\n",
       " ('created', 7),\n",
       " ('cheese', 7),\n",
       " ('Princess', 7),\n",
       " ('losing', 7),\n",
       " ('golden', 7),\n",
       " ('another', 7),\n",
       " ('a-list', 7),\n",
       " ('career', 7),\n",
       " ('park', 7),\n",
       " ('avoid', 7),\n",
       " ('case', 7),\n",
       " ('raises', 7),\n",
       " ('damaged', 7),\n",
       " ('Team', 7),\n",
       " ('party', 7),\n",
       " ('She', 7),\n",
       " ('Are', 7),\n",
       " ('Fancy', 7),\n",
       " ('pie', 7),\n",
       " ('One', 7),\n",
       " ('rate', 7),\n",
       " ('Do', 7),\n",
       " ('town', 7),\n",
       " ('abuse', 7),\n",
       " ('sets', 7),\n",
       " ('foot', 7),\n",
       " ('green', 7),\n",
       " ('rise', 7),\n",
       " ('1', 7),\n",
       " ('fall', 7),\n",
       " ('grand', 7),\n",
       " ('Wales', 7),\n",
       " ('group', 7),\n",
       " ('died', 7),\n",
       " ('youre', 7),\n",
       " ('shop', 7),\n",
       " ('pain', 7),\n",
       " ('named', 7),\n",
       " ('dies', 7),\n",
       " ('sell', 7),\n",
       " ('Russian', 7),\n",
       " ('virtual', 7),\n",
       " ('everything', 7),\n",
       " ('true', 7),\n",
       " ('Japan', 7),\n",
       " ('linked', 7),\n",
       " ('dressed', 7),\n",
       " ('stylish', 7),\n",
       " ('hidden', 7),\n",
       " ('accused', 7),\n",
       " ('Florida', 7),\n",
       " ('different', 7),\n",
       " ('building', 6),\n",
       " ('charged', 6),\n",
       " ('cash', 6),\n",
       " ('missing', 6),\n",
       " ('unable', 6),\n",
       " ('instead', 6),\n",
       " ('radio', 6),\n",
       " ('above', 6),\n",
       " ('If', 6),\n",
       " ('Mrs', 6),\n",
       " ('dramatic', 6),\n",
       " ('Barcelona', 6),\n",
       " ('confirms', 6),\n",
       " ('fitness', 6),\n",
       " ('midfielder', 6),\n",
       " ('Couple', 6),\n",
       " ('each', 6),\n",
       " ('teenagers', 6),\n",
       " ('track', 6),\n",
       " ('sells', 6),\n",
       " ('along', 6),\n",
       " ('rooms', 6),\n",
       " ('desert', 6),\n",
       " ('deadly', 6),\n",
       " ('worth', 6),\n",
       " ('poisoning', 6),\n",
       " ('forget', 6),\n",
       " ('25', 6),\n",
       " ('You', 6),\n",
       " ('host', 6),\n",
       " ('near', 6),\n",
       " ('popular', 6),\n",
       " ('nude', 6),\n",
       " ('staff', 6),\n",
       " ('cause', 6),\n",
       " ('calling', 6),\n",
       " ('China', 6),\n",
       " ('pair', 6),\n",
       " ('Samsung', 6),\n",
       " ('room', 6),\n",
       " ('enjoy', 6),\n",
       " ('drop', 6),\n",
       " ('smile', 6),\n",
       " ('friend', 6),\n",
       " ('Lancaster', 6),\n",
       " ('late', 6),\n",
       " ('make-up', 6),\n",
       " ('cheap', 6),\n",
       " ('Hollywood', 6),\n",
       " ('bath', 6),\n",
       " ('keeps', 6),\n",
       " ('As', 6),\n",
       " ('research', 6),\n",
       " ('d', 6),\n",
       " ('fear', 6),\n",
       " ('Paul', 6),\n",
       " ('father', 6),\n",
       " ('convicted', 6),\n",
       " ('Evans', 6),\n",
       " ('ultimate', 6),\n",
       " ('prices', 6),\n",
       " ('debut', 6),\n",
       " ('racist', 6),\n",
       " ('Fa', 6),\n",
       " ('thin', 6),\n",
       " ('male', 6),\n",
       " ('heads', 6),\n",
       " ('jet', 6),\n",
       " ('Charles', 6),\n",
       " ('amid', 6),\n",
       " ('champions', 6),\n",
       " ('ruins', 6),\n",
       " ('Map', 6),\n",
       " ('ft', 6),\n",
       " ('defeat', 6),\n",
       " ('less', 6),\n",
       " ('Japanese', 6),\n",
       " ('international', 6),\n",
       " ('Cambridge', 6),\n",
       " ('glamorous', 6),\n",
       " ('bed', 6),\n",
       " ('salad', 6),\n",
       " ('hope', 6),\n",
       " ('extraordinary', 6),\n",
       " ('centre', 6),\n",
       " ('beat', 6),\n",
       " ('Former', 6),\n",
       " ('fears', 6),\n",
       " ('Chef', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 1151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.Label.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1975"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9505 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 9000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "#texts = train.Headline.append(test.Headline).reindex()\n",
    "texts = df.no_stopwords\n",
    "\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "tokenizer = Tokenizer() #num_words=MAX_NB_WORDS\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2267, 30)\n",
      "Shape of label tensor: (2040, 1)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "\n",
    "test_data = data[train_index:]\n",
    "data = data[:train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = defaultdict(int)\n",
    "for number,item in enumerate(df.Category.unique()):\n",
    "    category_dict[item] = number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category_number'] = df['Category'].apply(lambda x : category_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_category = df.Category_number[indices][:-nb_validation_samples]\n",
    "x_val_category = df.Category_number[indices][-nb_validation_samples:]\n",
    "x_test_category = df.Category_number[train_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756     5\n",
       "1109    6\n",
       "121     5\n",
       "1293    7\n",
       "278     8\n",
       "       ..\n",
       "584     2\n",
       "630     7\n",
       "1696    2\n",
       "1444    6\n",
       "401     8\n",
       "Name: Category_number, Length: 1836, dtype: int64"
      ]
     },
     "execution_count": 1161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 25\n",
    "x_train_category = tf.one_hot(x_train_category, depth=depth)\n",
    "x_val_category = tf.one_hot(x_val_category, depth=depth)\n",
    "x_test_category = tf.one_hot(x_test_category, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(204, 1), dtype=float32, numpy=\n",
       "array([[3.3333333],\n",
       "       [3.3333333],\n",
       "       [2.       ],\n",
       "       [3.6666667],\n",
       "       [4.       ],\n",
       "       [4.       ],\n",
       "       [2.6666667],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [2.5      ],\n",
       "       [2.       ],\n",
       "       [3.6666667],\n",
       "       [2.       ],\n",
       "       [4.5      ],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [2.       ],\n",
       "       [4.3333335],\n",
       "       [5.       ],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [2.6666667],\n",
       "       [3.6666667],\n",
       "       [2.6666667],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [3.6666667],\n",
       "       [2.3333333],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [3.3333333],\n",
       "       [1.6666666],\n",
       "       [4.       ],\n",
       "       [2.       ],\n",
       "       [2.6666667],\n",
       "       [4.3333335],\n",
       "       [2.       ],\n",
       "       [2.3333333],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [2.6666667],\n",
       "       [2.       ],\n",
       "       [4.       ],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [2.5      ],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.6666667],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [4.3333335],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [4.5      ],\n",
       "       [2.       ],\n",
       "       [3.5      ],\n",
       "       [2.       ],\n",
       "       [2.6666667],\n",
       "       [4.       ],\n",
       "       [2.3333333],\n",
       "       [3.3333333],\n",
       "       [3.6666667],\n",
       "       [2.       ],\n",
       "       [4.5      ],\n",
       "       [2.       ],\n",
       "       [2.6666667],\n",
       "       [2.       ],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [4.3333335],\n",
       "       [3.       ],\n",
       "       [3.6666667],\n",
       "       [2.6666667],\n",
       "       [2.6666667],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [1.5      ],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.3333333],\n",
       "       [2.6666667],\n",
       "       [4.3333335],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [3.6666667],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [4.       ],\n",
       "       [3.3333333],\n",
       "       [3.3333333],\n",
       "       [2.3333333],\n",
       "       [2.6666667],\n",
       "       [4.       ],\n",
       "       [3.3333333],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [1.6666666],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [2.       ],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [2.       ],\n",
       "       [3.       ],\n",
       "       [2.       ],\n",
       "       [4.       ],\n",
       "       [2.3333333],\n",
       "       [4.3333335],\n",
       "       [4.6666665],\n",
       "       [2.       ],\n",
       "       [3.6666667],\n",
       "       [3.6666667],\n",
       "       [3.6666667],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.       ],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [2.3333333],\n",
       "       [3.6666667],\n",
       "       [4.       ],\n",
       "       [3.6666667],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [4.5      ],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [3.3333333],\n",
       "       [3.3333333],\n",
       "       [2.6666667],\n",
       "       [4.       ],\n",
       "       [2.3333333],\n",
       "       [4.       ],\n",
       "       [3.       ],\n",
       "       [4.       ],\n",
       "       [4.       ],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [1.6666666],\n",
       "       [2.3333333],\n",
       "       [3.6666667],\n",
       "       [4.5      ],\n",
       "       [4.       ],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [3.6666667],\n",
       "       [3.       ],\n",
       "       [3.6666667],\n",
       "       [4.       ],\n",
       "       [4.5      ],\n",
       "       [2.3333333],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [2.       ],\n",
       "       [1.6666666],\n",
       "       [4.       ],\n",
       "       [3.6666667],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [4.       ],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [4.3333335],\n",
       "       [3.3333333],\n",
       "       [4.3333335],\n",
       "       [3.3333333],\n",
       "       [4.       ],\n",
       "       [2.6666667],\n",
       "       [3.5      ],\n",
       "       [2.       ],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.3333333],\n",
       "       [3.       ],\n",
       "       [3.       ],\n",
       "       [2.6666667],\n",
       "       [4.       ],\n",
       "       [3.       ],\n",
       "       [3.3333333],\n",
       "       [3.6666667]], dtype=float32)>"
      ]
     },
     "execution_count": 1164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196016 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = os.getcwd()\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moneymillion\n",
      "瞿\n",
      "sensatori\n",
      "colborn\n",
      "wetherspoon\n",
      "straubenzee\n",
      "kassig\n",
      "bloggling\n",
      "casilli\n",
      "washoku\n",
      "kalette\n",
      "ratajkowski\n",
      "mcconnel\n",
      "schapelle\n",
      "manorexia\n",
      "zmapp\n",
      "ntas\n",
      "williamsfield\n",
      "tressful\n",
      "marcu\n",
      "radamel\n",
      "hitchbot\n",
      "143840\n",
      "blueberrie\n",
      "satorova\n",
      "parahawking\n",
      "lasource\n",
      "gignac\n",
      "vvb\n",
      "taylforth\n",
      "lannisters\n",
      "versini\n",
      "263000\n",
      "leonore\n",
      "kintbury\n",
      "278000\n",
      "latelier\n",
      "shooters4\n",
      "flumps\n",
      "sraphine\n",
      "solanke\n",
      "silbury\n",
      "marmadukes\n",
      "92million\n",
      "ossetra\n",
      "sølveig\n",
      "urfboard\n",
      "clooneys\n",
      "huguette\n",
      "pogba\n",
      "crabzilla\n",
      "kuratas\n",
      "orionid\n",
      "pedraza\n",
      "seewald\n",
      "lovren\n",
      "jambos\n",
      "duffen\n",
      "gunton\n",
      "udons\n",
      "headguards\n",
      "leiby\n",
      "kletzky\n",
      "aisleyne\n",
      "bregancon\n",
      "103f\n",
      "hudl\n",
      "endage\n",
      "mowatt\n",
      "megaburgerpizza\n",
      "mucklow\n",
      "exually\n",
      "hockingly\n",
      "hinxton\n",
      "colchicums\n",
      "bildeston\n",
      "megeve\n",
      "alamuddin\n",
      "robach\n",
      "downtons\n",
      "huashan\n",
      "ledoyen\n",
      "graubunden\n",
      "wagguccinos\n",
      "revoluntionise\n",
      "swaffham\n",
      "gavroche\n",
      "calded\n",
      "ötzi\n",
      "agdal\n",
      "battersby\n",
      "ramesses\n",
      "mh370\n",
      "tarfish\n",
      "twivial\n",
      "bellecote\n",
      "bennewith\n",
      "jezki\n",
      "argentinosaurus\n",
      "marouane\n",
      "hadza\n",
      "eyeteq\n",
      "honut\n",
      "antikythera\n",
      "mcvitie\n",
      "madagascan\n",
      "yarnold\n",
      "yobaba\n",
      "lyth\n",
      "healthkit\n",
      "ershver\n",
      "tooni\n",
      "monhrr\n",
      "uperlanguage\n",
      "lopilato\n",
      "ad70\n",
      "sweetroot\n",
      "a46\n",
      "sportsmail\n",
      "bafetimbi\n",
      "gomis\n",
      "willerton\n",
      "priceles\n",
      "humphrys\n",
      "baynes\n",
      "instagrammers\n",
      "uperpower\n",
      "touessrok\n",
      "guman\n",
      "monis\n",
      "chemmy\n",
      "ampika\n",
      "pickston\n",
      "backgrounto\n",
      "thoren\n",
      "litvinenko\n",
      "minoans\n",
      "dymphna\n",
      "enner\n",
      "moët\n",
      "twinstagram\n",
      "froome\n",
      "hyperloop\n",
      "mccoist\n",
      "evertomb\n",
      "weligton\n",
      "uicidal\n",
      "mh17\n",
      "gredos\n",
      "woolton\n",
      "gwynnie\n",
      "searcys\n",
      "mimas\n",
      "ickening\n",
      "foodstagrammer\n",
      "urprise\n",
      "missionarie\n",
      "brelfie\n",
      "plebgate\n",
      "ubversive\n",
      "mertesacker\n",
      "fifpro\n",
      "mbye\n",
      "bishoo\n",
      "ardche\n",
      "derryn\n",
      "flakka\n",
      "altamura\n",
      "skuse\n",
      "chubbuck\n",
      "railton\n",
      "cdbury\n",
      "mcerlane\n",
      "u21s\n",
      "proudlock\n",
      "scudamore\n",
      "aqap\n",
      "gnter\n",
      "leeuwenhoek\n",
      "gizzi\n",
      "maksims\n",
      "uvarenko\n",
      "salzberg\n",
      "crapser\n",
      "martinhal\n",
      "mckendrick\n",
      "whitear\n",
      "isogawa\n",
      "yazidi\n",
      "eurocamp\n",
      "argeles\n",
      "idebar\n",
      "verasamy\n"
     ]
    }
   ],
   "source": [
    "#Glove\n",
    "EMBEDDING_DIM = embeddings_index['car'].shape[0]\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word,i in word_index.items():\n",
    "    #if i < num_words:\n",
    "        #print(word)\n",
    "        emb_vec=embeddings_index.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i]=emb_vec\n",
    "            #print(i)\n",
    "        else: print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_custom_accuracy_function', patience = 5)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 為該層建立一個可訓練的權重\n",
    "        #inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最後呼叫它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "\n",
    "\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "\n",
    "        QK = QK / (64**0.5)\n",
    "\n",
    "        QK = K.softmax(QK)\n",
    "\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "\n",
    "        V = K.batch_dot(QK,WV)\n",
    "\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "    #weight = np.array([1,2,3,4,5],dtype='f').reshape(-1,1)\n",
    "    #weight = tf.convert_to_tensor(weight)\n",
    "    #weight = tf.Variable(np.array([1,2,3,4,5],dtype='f').reshape(-1,1), dtype=tf.float32)\n",
    "    #squared_difference = tf.square(y_true - tf.matmul(y_pred,weight)) #+tf.square(5 - tf.matmul(y_pred,weight))+tf.square(1 - tf.matmul(y_pred,weight))\n",
    "    #print(tf.square(3.15 - tf.matmul(y_pred,weight))[0])\n",
    "    squared_difference = tf.square(y_true - y_pred)+0.1*tf.square(1 - y_pred)+0.1*tf.square(5 - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "\n",
    "def custom_accuracy_function(y_true, y_pred):\n",
    "    #if tf.math.argmax(y_true,1)-tf.math.argmax(y_pred,1)==0:\n",
    "        #a = 1 \n",
    "    a=abs(y_true-y_pred)<0.5\n",
    "    return K.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words=len(word_index)+1\n",
    "\n",
    "embedding = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqSelfAttention,SeqWeightedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ.shape (None, 30, 128)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 128, 30)\n",
      "QK.shape (None, 30, 30)\n"
     ]
    }
   ],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "category_input = Input(shape=(depth,), dtype='float32')\n",
    "embedded_sequences = embedding(sequence_input)\n",
    "embed_self = Self_Attention(128)(embedded_sequences)\n",
    "#a1 = Conv1D(32, 3, activation='relu')(embed_self)\n",
    "#a2 = Conv1D(32, 3, activation='relu')(embedded_sequences)\n",
    "#a = concatenate([a1, a2], axis=1)\n",
    "#a = MaxPooling1D(2)(a1)\n",
    "#b1 = Conv1D(32, 4, activation='relu')(embed_self)\n",
    "#b2 = Conv1D(32, 4, activation='relu')(embedded_sequences)\n",
    "#b = concatenate([b1, b2], axis=1)\n",
    "#b = MaxPooling1D(2)(b1) # global max pooling\n",
    "#c = Conv1D(32, 5, activation='relu')(embedded_sequences)\n",
    "#c = MaxPooling1D(2)(c)\n",
    "#x = concatenate([a, b], axis=1)\n",
    "\n",
    "#x = Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2))(embed_self)\n",
    "x = LSTM(128, dropout=0.2,recurrent_dropout=0.2)(embed_self)\n",
    "#x = LSTM(64, dropout=0.2,recurrent_dropout=0.2)(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "x = concatenate([x,category_input])\n",
    "#x = Flatten()(x)\n",
    "#x = Dense(100,kernel_initializer='random_uniform', activation='relu')(x)\n",
    "x = Dense(32,kernel_initializer='random_uniform', activation='relu')(x)\n",
    "\n",
    "preds = Dense(1)(x)\n",
    "\n",
    "optimzer=Adam(learning_rate=1e-4)\n",
    "\n",
    "model = Model([sequence_input,category_input], preds)\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=optimzer,\n",
    "              metrics=[custom_accuracy_function])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_149\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_161 (InputLayer)          [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 30, 300)      2851800     input_161[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_66 (Self_Attent (None, 30, 128)      115200      embedding_25[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_122 (LSTM)                 (None, 128)          131584      self__attention_66[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input_162 (InputLayer)          [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_104 (Concatenate)   (None, 153)          0           lstm_122[0][0]                   \n",
      "                                                                 input_162[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_198 (Dense)               (None, 32)           4928        concatenate_104[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_199 (Dense)               (None, 1)            33          dense_198[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,103,545\n",
      "Trainable params: 251,745\n",
      "Non-trainable params: 2,851,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WQ.shape (None, 30, 128)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 128, 30)\n",
      "QK.shape (None, 30, 30)\n",
      "WQ.shape (None, 30, 128)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 128, 30)\n",
      "QK.shape (None, 30, 30)\n",
      "58/58 [==============================] - ETA: 0s - loss: 8.2303 - custom_accuracy_function: 0.0102WQ.shape (None, 30, 128)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 128, 30)\n",
      "QK.shape (None, 30, 30)\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 8.2303 - custom_accuracy_function: 0.0102 - val_loss: 1.3080 - val_custom_accuracy_function: 0.2455\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.5993 - custom_accuracy_function: 0.4625 - val_loss: 0.4825 - val_custom_accuracy_function: 0.5283\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.5132 - custom_accuracy_function: 0.4966 - val_loss: 0.4844 - val_custom_accuracy_function: 0.5357\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.5043 - custom_accuracy_function: 0.4883 - val_loss: 0.4764 - val_custom_accuracy_function: 0.5402\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4939 - custom_accuracy_function: 0.4939 - val_loss: 0.4547 - val_custom_accuracy_function: 0.5536\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4830 - custom_accuracy_function: 0.5040 - val_loss: 0.4502 - val_custom_accuracy_function: 0.5372\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4741 - custom_accuracy_function: 0.5077 - val_loss: 0.4467 - val_custom_accuracy_function: 0.5461\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4641 - custom_accuracy_function: 0.5282 - val_loss: 0.4319 - val_custom_accuracy_function: 0.5506\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4555 - custom_accuracy_function: 0.5208 - val_loss: 0.4464 - val_custom_accuracy_function: 0.5253\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 4s 67ms/step - loss: 0.4484 - custom_accuracy_function: 0.5323 - val_loss: 0.4227 - val_custom_accuracy_function: 0.5461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f64970ec050>"
      ]
     },
     "execution_count": 1179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, x_train_category], y_train, validation_data=([x_val, x_val_category], y_val),\n",
    "          epochs=100, batch_size=32, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ.shape (None, 30, 128)\n",
      "K.permute_dimensions(WK, [0, 2, 1]).shape (None, 128, 30)\n",
      "QK.shape (None, 30, 30)\n"
     ]
    }
   ],
   "source": [
    "y_val_pre = model.predict([x_val,x_val_category])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.42274448], dtype=float32)>"
      ]
     },
     "execution_count": 1181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((y_val - y_val_pre)**2 )/ len(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<0.1% Accuracy = [0.12745099]\n",
      "<0.2% Accuracy = [0.25]\n",
      "<0.3% Accuracy = [0.32843137]\n",
      "<0.4% Accuracy = [0.42647058]\n",
      "<0.5% Accuracy = [0.53431374]\n"
     ]
    }
   ],
   "source": [
    "threshold_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "for threshold in threshold_list:\n",
    "    \n",
    "    accuracy = sum(tf.cast((abs(y_val - y_val_pre)<threshold), tf.float32)) / len(y_val)\n",
    "    print('<{}% Accuracy = {}'.format(threshold, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub=pd.read_csv('sampleSubmission.csv')\n",
    "sample_sub = sample_sub.sort_values(by=['ID'])\n",
    "y_pre=model.predict([test_data,x_test_category]) #.dot(np.array([1,2,3,4,5]).reshape(-1,1))\n",
    "sub=pd.DataFrame({'ID':sample_sub['ID'].values.tolist(),'Label':y_pre.reshape(-1,)})\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9033868 3.469649\n"
     ]
    }
   ],
   "source": [
    "print(y_pre.min(),y_pre.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
